{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "from transformers import HfArgumentParser, AutoTokenizer, PreTrainedTokenizerFast\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Arguments\n",
    "from logger_config import logger\n",
    "from data_utils import log_random_samples, load_corpus, format_documents_for_final_answer\n",
    "from vllm_client_local import VllmClient, get_vllm_model_id\n",
    "from utils import save_json_to_file, AtomicCounter\n",
    "from agent import CoRagAgent, RagPath\n",
    "from inference.metrics import compute_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_ip = \"10.197.17.39\"\n",
    "e5_ip = \"10.197.17.38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 15:16:41,024 INFO] HTTP Request: GET http://10.197.17.39:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b145e31d3f4e829952eecf6962e8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ee5a1b01e047e8b62f91040ceb1be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a491d6405c904115ba0f000c59c12b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 15:29:02,658 INFO] Loaded 35678076 passages from corag/kilt-corpus\n"
     ]
    }
   ],
   "source": [
    "vllm_client: VllmClient = VllmClient(get_vllm_model_id(host=vllm_ip), host=vllm_ip)\n",
    "corpus: Dataset = load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-07 22:49:30,482 INFO] HTTP Request: GET http://10.197.17.39:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "corag_agent: CoRagAgent = CoRagAgent(vllm_client=vllm_client, corpus=corpus, e5_ip=e5_ip, vllm_ip=vllm_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from agent.agent_utils import RagPath\n",
    "from openai.types.chat import ChatCompletion\n",
    "from prompts import get_generate_subquery_prompt, get_generate_intermediate_answer_prompt, get_generate_final_answer_prompt\n",
    "\n",
    "from agent.corag_agent import CoRagAgent\n",
    "from agent.corag_agent import _normalize_subquery\n",
    "\n",
    "from vllm_client_local import VllmClient\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, path: RagPath, logprob: float, parent: Optional[\"TreeNode\"] = None):\n",
    "        self.path = deepcopy(path)\n",
    "        self.logprob = logprob  # cumulative log probability of subqueries\n",
    "        self.depth = len(path.past_subqueries)\n",
    "        self.parent = parent\n",
    "        self.levin_cost = 0.0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.levin_cost < other.levin_cost\n",
    "    \n",
    "class CoRagAgentWithPHS(CoRagAgent):\n",
    "    \n",
    "    def __init__(self, vllm_client: VllmClient, corpus: Dataset, e5_ip: str, vllm_ip: str, \n",
    "                 confidence_threshold: float = 0.5):\n",
    "        \"\"\"Initializes the CoRagAgentWithPHS class.\n",
    "        This class is a specialized version of the CoRagAgent that implements a tree search algorithm\n",
    "        called 'Policy-guided heuristic search' (PHS) to quickly find a good path for answering a query.\n",
    "\n",
    "        Args:\n",
    "            vllm_client (VllmClient): VLLM client for answering queries.\n",
    "            corpus (Dataset): Dataset containing the documents to be searched.\n",
    "            e5_ip (str): IP address of the E5 server.\n",
    "            vllm_ip (str): IP address of the VLLM server.\n",
    "            confidence_threshold (float, optional): Confidence threshold to determine if completed. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        super().__init__(vllm_client, corpus, e5_ip, vllm_ip)\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "    def tree_search(\n",
    "        self, query: str, \n",
    "        task_desc: str,\n",
    "        max_path_length: int = 3,\n",
    "        max_message_length: int = 4096,\n",
    "        temperature: float = 0.7,\n",
    "        expand_size: int = 4,\n",
    "        max_tree_size = 100,\n",
    "        **kwargs\n",
    "    ) -> RagPath:\n",
    "        root_path = RagPath(query=query, past_subqueries=[], past_subanswers=[], past_doc_ids=[])\n",
    "        root_node = TreeNode(path=root_path, logprob=0.0)\n",
    "\n",
    "        open_list = []\n",
    "        heapq.heappush(open_list, root_node)\n",
    "        explored_num = 0\n",
    "        while open_list and explored_num < max_tree_size:\n",
    "            explored_num += 1\n",
    "            if explored_num % 10 == 0:\n",
    "                logger.info(f\"Explored nodes: {explored_num}\")\n",
    "            node = heapq.heappop(open_list)\n",
    "            current_path = node.path\n",
    "\n",
    "            if self._is_solution(current_path, task_desc, max_message_length):\n",
    "                return current_path\n",
    "\n",
    "            messages = get_generate_subquery_prompt(\n",
    "                query=query,\n",
    "                past_subqueries=current_path.past_subqueries,\n",
    "                past_subanswers=current_path.past_subanswers,\n",
    "                task_desc=task_desc\n",
    "            )\n",
    "            self._truncate_long_messages(messages, max_length=max_message_length)\n",
    "\n",
    "            completion: ChatCompletion = self.vllm_client.call_chat(\n",
    "                messages=messages,\n",
    "                return_str=False,\n",
    "                n=expand_size,\n",
    "                # extra_body={\"prompt_logprobs\": 1},\n",
    "                logprobs=True,\n",
    "                temperature=temperature,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            for choice in completion.choices:\n",
    "                subquery = _normalize_subquery(choice.message.content)\n",
    "                if subquery in current_path.past_subqueries:\n",
    "                    continue\n",
    "                \n",
    "                token_logprobs = [c.logprob for c in choice.logprobs.content]\n",
    "                \n",
    "                sub_logprob = sum(token_logprobs) / max(len(token_logprobs), 1)\n",
    "\n",
    "                subanswer, doc_ids = self._get_subanswer_and_doc_ids(\n",
    "                    subquery=subquery, max_message_length=max_message_length\n",
    "                )\n",
    "\n",
    "                new_path = RagPath(\n",
    "                    query=query,\n",
    "                    past_subqueries=current_path.past_subqueries + [subquery],\n",
    "                    past_subanswers=current_path.past_subanswers + [subanswer],\n",
    "                    past_doc_ids=current_path.past_doc_ids + [doc_ids]\n",
    "                )\n",
    "\n",
    "                running_cost = len(new_path.past_subqueries) # number of nodes (g(n) in the paper)\n",
    "                policy_logprob = node.logprob + sub_logprob # cumulative log probability (log(pi(n)) in the paper)\n",
    "                heuristic_cost = self._estimate_heuristic_llm(new_path, task_desc, max_message_length, **kwargs) # h(n) in the paper\n",
    "                levin_cost = math.log(running_cost + heuristic_cost + 1e-5) - policy_logprob\n",
    "\n",
    "                new_node = TreeNode(path=new_path, logprob=policy_logprob, parent=node)\n",
    "                new_node.levin_cost = levin_cost\n",
    "                heapq.heappush(open_list, new_node)\n",
    "\n",
    "        # use this only if nothing worked idk?? can use logger instead\n",
    "        print(f\"Did not find a solution within {max_tree_size} nodes. Returning the root path.\")\n",
    "        return self.sample_path(\n",
    "                    query=query,\n",
    "                    task_desc=task_desc,\n",
    "                    max_path_length=max_path_length,\n",
    "                    max_message_length=max_message_length,\n",
    "                    temperature=temperature,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "    def _is_solution(self, path: RagPath, task_desc: str, max_message_length: int) -> bool:\n",
    "        log_prob = self._eval_single_path(\n",
    "            path,\n",
    "            max_message_length=max_message_length\n",
    "        )\n",
    "        # This is the log probability of the string 'No relevant information found'\n",
    "        # So, if the log probability of the path is less than this, we consider it a solution.\n",
    "        # This is a heuristic, and the threshold can be adjusted based on the model's behavior.\n",
    "        return log_prob < math.log(self.confidence_threshold)\n",
    "\n",
    "    def _estimate_heuristic_llm(self, path: RagPath, task_desc: str, max_message_length: int, **kwargs) -> int:\n",
    "        \"\"\"Heuristic function to estimate the number of remaining subqueries.\n",
    "        This function uses the LLM to predict how many subqueries are needed to fully answer the original query.\n",
    "\n",
    "        Args:\n",
    "            path (RagPath): RagPath to the current node in the search tree\n",
    "            task_desc (str): Task description\n",
    "            max_message_length (int): Maximum message length for the LLM\n",
    "            **kwargs: Additional arguments for the LLM call\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated number of remaining subqueries\n",
    "        \"\"\"\n",
    "        # Ask the LLM how many subqueries it expects are remaining\n",
    "        messages: List[Dict] = get_generate_intermediate_answer_prompt(\n",
    "            subquery=path.query,\n",
    "            documents=[f'Q: {q}\\nA: {a}' for q, a in zip(path.past_subqueries, path.past_subanswers)],\n",
    "        )\n",
    "        # messages.append({'role': 'user', 'content': 'How many more subqueries are needed to fully answer the original query. Respond with a single integer.'})\n",
    "        messages.append({'role': 'user', \n",
    "                         'content': f'What subqueries should be asked to fully answer the original query: \"{path.query}\". Separate subqueries with question marks \"?\".'})\n",
    "        self._truncate_long_messages(messages, max_length=max_message_length)\n",
    "\n",
    "        response: ChatCompletion = self.vllm_client.call_chat(\n",
    "            messages=messages,\n",
    "            return_str=False,\n",
    "            # max_tokens=5,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        est_remaining = max(1,len(text.split(\"?\")))\n",
    "\n",
    "        # response: ChatCompletion = self.vllm_client.call_chat(\n",
    "        #     messages=messages,\n",
    "        #     return_str=False,\n",
    "        #     max_tokens=5,\n",
    "        #     **kwargs\n",
    "        # )\n",
    "\n",
    "        # # idk how to ensure its an int, this was an attempt\n",
    "        # try:\n",
    "        #     est_remaining = int(text.split()[0])\n",
    "        # except Exception:\n",
    "        #     est_remaining = max(1, 3 - len(path.past_subqueries))  # fallback\n",
    "\n",
    "        return max(0, est_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 15:32:08,004 INFO] HTTP Request: GET http://10.197.17.39:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(get_vllm_model_id(host=vllm_ip))\n",
    "tokenizer_lock: threading.Lock = threading.Lock()\n",
    "processed_cnt: AtomicCounter = AtomicCounter()\n",
    "total_cnt: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds: Dataset = load_dataset('corag/multihopqa', \"musique\", split=\"validation\")\n",
    "ds = ds.remove_columns([name for name in ['subqueries', 'subanswers', 'predictions'] if name in ds.column_names])\n",
    "ds = ds.add_column('task_desc', ['answer multi-hop questions' for _ in range(len(ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.select(range(16))\n",
    "args = Arguments()\n",
    "ex = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 16:03:17,680 INFO] HTTP Request: GET http://10.197.17.39:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "phs_agent = CoRagAgentWithPHS(\n",
    "    vllm_client=vllm_client,\n",
    "    corpus=corpus,\n",
    "    e5_ip=e5_ip,\n",
    "    vllm_ip=vllm_ip,\n",
    "    confidence_threshold=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 16:03:20,430 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:20,762 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:21,134 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:21,567 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:21,786 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:22,118 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:22,336 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:22,871 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:23,091 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:23,625 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:23,663 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:23,834 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:23,872 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,044 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,082 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,253 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,291 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,462 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:24,620 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find a solution within 50 nodes. Returning the root path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 16:03:24,874 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:25,032 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:25,189 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:25,346 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:25,504 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:25,691 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:26,062 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:26,224 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:26,367 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:26,715 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:26,863 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,020 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,177 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,320 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,477 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,634 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,791 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:27,933 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,091 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,247 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,406 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,549 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,692 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,835 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:28,977 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-09 16:03:29,120 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "path: RagPath = phs_agent.tree_search(\n",
    "        query=ex['query'], \n",
    "        task_desc=ex['task_desc'],\n",
    "        max_path_length=6,\n",
    "        temperature=0.2,\n",
    "        max_tree_size=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-07 17:27:01,817 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:02,093 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:02,251 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:02,394 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:02,719 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:02,864 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,019 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,175 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,317 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,458 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,614 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,770 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:03,911 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:04,053 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:04,512 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:04,656 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:04,798 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:04,939 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:05,081 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:05,222 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:05,364 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:05,506 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:05,840 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:06,012 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:06,678 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:06,823 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:06,965 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:07,108 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-04-07 17:27:07,250 INFO] HTTP Request: POST http://10.197.17.38:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "path: RagPath = corag_agent.sample_path(\n",
    "        query=ex['query'], \n",
    "        task_desc=ex['task_desc'],\n",
    "        max_path_length=6,\n",
    "        temperature=0., \n",
    "        max_tokens=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[str] = format_documents_for_final_answer(\n",
    "    args=args,\n",
    "    context_doc_ids=ex['context_doc_ids'],\n",
    "    tokenizer=tokenizer, corpus=corpus,\n",
    "    lock=tokenizer_lock\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-09 15:34:31,439 INFO] HTTP Request: POST http://10.197.17.39:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "prediction: str = phs_agent.generate_final_answer(\n",
    "        corag_sample=path,\n",
    "        task_desc=ex['task_desc'],\n",
    "        documents=documents,\n",
    "        max_message_length=args.max_len,\n",
    "        temperature=0., max_tokens=128\n",
    "    )\n",
    "\n",
    "ex_with_path = copy.deepcopy(ex)\n",
    "ex_with_path['subqueries'] = path.past_subqueries\n",
    "ex_with_path['subanswers'] = path.past_subanswers\n",
    "ex_with_path['path_doc_ids'] = path.past_doc_ids\n",
    "if 'title' in corpus.column_names:\n",
    "    ex_with_path['path_doc_titles'] = [\n",
    "        [corpus[int(doc_id)]['title'] for doc_id in doc_ids] for doc_ids in path.past_doc_ids\n",
    "    ]\n",
    "ex_with_path['prediction'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the name of the Green performer?',\n",
       " 'Is the Green performer male?',\n",
       " 'Is the Green performer married?',\n",
       " 'Is the Green performer married to a woman?',\n",
       " 'Is the Green performer married to a woman named Sarah?',\n",
       " \"What is the name of the Green performer's spouse?\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_with_path['subqueries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No relevant information found',\n",
       " 'No relevant information found.',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No relevant information found.',\n",
       " 'Laura Bayley']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_with_path['subanswers']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
